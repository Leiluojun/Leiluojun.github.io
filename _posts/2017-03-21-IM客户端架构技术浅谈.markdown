---
layout:     post
title:      "IM客户端架构技术浅谈"
subtitle:   ""
date:       2017-03-21 13:05:14
author:     "Yuhj"
header-img: "img/post-bg-nextgen-web-pwa.jpg"
header-mask: 0.3
catalog:    true
description: 本人之前从事过一段时间Android端IM SDK的开发，期间也做了许多技术调研以及踩了很多坑，现在我统一介绍下一个IM APP的方方面面，包括技术选型（包括通讯方式，网络连接方式，协议选择）和常见问题。
tags:
    - Android
    - IM
    
---
## 前言
本人之前从事过一段时间Android端IM SDK的开发，期间也做了许多技术调研以及踩了很多坑，现在我统一介绍下一个IM APP的方方面面，包括技术选型（包括通讯方式，网络连接方式，协议选择）和常见问题。浅薄之见，望大家别见笑，欢迎给出批评意见。
![](/image/im/im.png)
上图是一个IM APP所用到的常用技术点。
## P2P还是服务器中转？
IM通讯方式无非两种选择：设备直连(P2P)和通过服务器中转。
#### P2P方式
##### P2P多见于局域网内聊天工具，典型的应用有:飞鸽传书、天网Maze等。这类软件在启动后一般做两件事情:
- 进行UDP广播:发送自己信息和接受同局域网内其他端信息；
- 开启TCP监听:等待其他端进行连接。<br>

详细的流程可以参考飞鸽传书源码。但是这种方式在有种种限制和不便:一方面它只适合在线的点对点消息传输，对离线，群组等业务支持不够。另一方面由于 NAT 的存在，使得不同局域网内机器互联难度大大上升，在某些网络类型(对称NAT)下无法建立连接。
## IM到底该用UDP还是TCP协议？
说到IM该用UDP还是TCP作为传输协议，这是个颇有争议的话题，各大社区每当此问题的出现必定是大片的不同声音。<br>

当然，UDP和TCP各有各的应用场景，作为IM来说，早期的IM因为服务端资源（服务器硬件、网络带宽等）比较昂贵且没有更好的办法来分担性能负载，所以很多时候会考虑使用UDP，这其中主要是早期的QQ为代表。

时至今日，TCP的服务端负载已经有了很好的解决方案，加之服务器资源成本的下降，目前很多IM、消息推送解决方案也都在使用TCP作为传输层协议。不过，UDP也并未排除在IM、消息推送的解决方案之外，比如：弱网络通信（包括跨国的高延迟网络环境）、物联网通信、IM中的实时音视频通信等等场景下，UDP依然是首选项(比如飞信)。


## 该选择什么样的网络通讯技术？
#### IM主流网络通讯技术有两种:
- 基于TCP的长连接；
- 基于HTTP短连接PULL的方式。

后者常见于WEB IM系统(当然现在很多WEB IM都是基于WebSocket实现)，它的优点是实现简单，方便开发上手，问题是流量大，服务器负载较大，消息及时性无法很好地保证，对大规模的用户量支持不够，比较适合小型的IM系统,如小网站的客户系统。<br>

基于TCP长连接则能够更好地支持大批量用户，问题是客户端和服务器的实现比较复杂。

## 协议如何制定？
IM协议选择原则一般是：易于拓展，方便覆盖各种业务逻辑，同时又比较节约流量。后一点的需求在移动端IM上尤其重要。常见的协议有：XMPP、MQTT、私有协议。
#### XMPP
- 优点：协议开源，可拓展性强，在各个端(包括服务器)有各种语言的实现，开发者接入方便；
- 缺点：缺点也是不少，XML表现力弱、有太多冗余信息、流量大，实际使用时有大量天坑。

很多开发者为了节省开发成本，大多选择了XMPP协议。
#### MQTT
- 优点：协议简单，流量少；
- 缺点：它并不是一个专门为IM设计的协议，多使用于推送。

#### 私有协议
- 优点：高效，节约流量(一般使用二进制协议)，安全性高，难以破解；
- 缺点：在开发初期没有现有样列可以参考，对于设计者的要求比较高。

市面上几乎所有主流IM SDK都是是使用私有协议，一个被良好设计的私有协议优点非常明显。
#### 结论
一个好的协议需要满足如下条件:高效，简洁，可读性好，节约流量，易于拓展，同时又能够匹配当前团队的技术堆栈。基于如上原则，我们可以得出:如果团队小，团队技术在IM上积累不够可以考虑使用XMPP或者MQTT+HTTP短连接的实现。反之可以考虑自己设计和实现私有协议

## 该如何设计私有通信协议？
#### 序列化与反序列化
移动互联网相对于有线网络最大特点是:带宽低，延迟高，丢包率高和稳定性差，流量费用高。所以在私有协议的序列化上一般使用二进制协议，而不是文本协议。

常见的二进制序列化库有`protobuf`和`MessagePack`，当然你也可以自己实现自己的二进制协议序列化和反序列的过程，比如蘑菇街的TeamTalk。但是前面二者无论是可拓展性还是可读性都非常优秀，所以大部分情况下还是不推荐自己去实现二进制协议的序列化和反序列化过程。

#### 强列建议使用Protobuf，理由如下：
- `灵活、高效`：灵活（方便接口更新）、高效（效率经过google的优化，传输效率比普通的XML等高很多）；
- `易于使用`：开发人员通过按照一定的语法定义结构化的消息格式，然后送给命令行工具，工具将自动生成相关的类，可以支持java、c++、python等语言环境。通过将这些类包含在项目中，可以很轻松的调用相关方法来完成业务消息的序列化与反序列化工作。
- `语言支持`：原生支持c++、java、python等多达10余种语言。

一条消息数据用Protobuf序列化后的大小是JSON的1/10、XML格式的1/20、是二进制序列化的1/10。

#### 协议格式设计
基于TCP的应用层协议一般都分为包头和包体(如HTTP)，IM协议也不例外。包头一般用于表示每个请求/反馈的公共部分，如包长，请求类型，返回码等。 而包头则填充不同请求/反馈对应的信息。<br>
```java
    public class ClientBytesParser {   
    /**
     *  协议头（2 byte）：
     */
    public static final int PROTOCOL_HEADRE;   
    /**
     * 0x01 （目前为第一个版本）
     */
    public static final int PROTOCOL_VERSION  ;   
    /**
     * 比特标志位（1 byte）： 0 ack，是否需要消息回执 1 zip，body是否压缩 2~7为保留位  
    public static final int PROTOCOL_MRAK;
     /**
     * 表示请求类型或body的类型
     */
    public static final int PROTOCOL_TYPE;
    /**
     * length（2 byte）：body长度
     */
    public static final int PROTOCOL_LENGTH;
    /**
     * RPC （2 byte）：rpc请求的id，需要对方原样返回
     */
    public static final int PROTOCOL_RPC;
```  
    
协议格式可以根据各自的业务需求设置不同，但统一的原则是尽可能的精简，以减少请求的数据量。

## 其它不可忽视的问题
上面的内容就是一个IM系统大致的选型过程：服务方式，网络通讯协议，数据通信协议选择、协议设计。但是实际开发过程中还有大量的问题需要处理。其中如果保证链接的`稳定性`和消息到达的`可靠性`是重中之重。
#### 协议加密
为了保证协议不容易被破解，市面上几乎所有主流IM都会对协议进行加密传输。常见的流程和HTTPS加密相似:建立连接后，客户端和服务器进行进行协商，最终客户端获得一个当前Sessino的秘钥，后续的数据传输都通过这个秘钥进行加解密。一般出于效率的考虑都会采用流式加密，如RC4。具体的加密算法可根据业务需求选择不同难度的加密算法。
#### 快速连接（即掉线重连机制）
对于一个长链接而言，永远保持连接状态几乎不可能，APP每次启动基本都需要一次重连登录(短时间内切换除外)，所以如何快速重连、重登就非常重要。
##### 常见优化思路如下:
- 本地缓存服务器IP并定期刷新。
- 合并部分请求。如加密和登录操作可以合并为同一个操作，这样就可以减少一次不必要的网络请求来回的时间；
- 简化登录后的同步请求，部分同步请求可以推迟到UI操作时进行，如用户信息刷新。

其中合并部分网络请求在IM中很常用到，这就是大家常听到的网络请求的`分包`与`合包`;但客户端频繁的像服务器发送网络请求时，为了避免网络请求回来的时间以及额外增加的包（每次请求都会带固定的包头），客户端会把多次请求合并，具体每次请求的包大小上限可根据服务器性能以及用户网络状态动态调整。同样，服务器在推送消息时也会对网络请求进行合包，此时客户端需要分包。 
```java
/**
     * 处理剩下的数据
     *
     * @param data
     */
    private void handleLeftData(byte[] data) {
        if (data != null && data.length > 0) {
            int position = mTransferData.getPosition();
            int length = data.length;
            byte dst[] = new byte[position + length];
            // 将上次不完整数据跟本次数据合并
            System.arraycopy(mTransferData.getData(), 0, dst, 0, position);
            System.arraycopy(data, 0, dst, position, length);
            mTransferData.init();
            handleData(dst);
        }
    }   
   /**
     * 如果数据完成回调回去，否则缓存等待数据完整
     *
     * @param data
     */
    private void handleData(byte[] data) {
        if (data != null && data.length > 0) {
            boolean isEffectData = ClientBytesParser.isEffectData(data);
            if (data.length > ClientBytesParser.getHeartBytes().length
                    && isEffectData) {
                int length = ClientBytesParser.getLength(data);
                int type = ClientBytesParser.getType(data);
                int rpc = ClientBytesParser.getRPC(data);
                boolean zip = ClientBytesParser.isZip(data);
                int total = HEADRE_LENGTH + length;
                mTransferData.setLength(length);
                mTransferData.setRpc(rpc);
                mTransferData.setType(type);
                // 头+body
                mTransferData.setData(new byte[total]);
                int cLength = total >= data.length ? data.length : total;
                mTransferData.setPosition(cLength);
                System.arraycopy(data, 0, mTransferData.getData(), 0, cLength);
                // 如果buffer存的下发送的所有数据
                if (total <= data.length) {
                    OnResponseListener listener = ResponseDispacher
                            .getResponseListener(String.valueOf(rpc));
                    // 去掉头（body）
                    byte[] arr = new byte[length];
                    System.arraycopy(data, HEADRE_LENGTH, arr, 0, length);
                    if (zip) {
                        mTransferData.setData(ClientBytesParser.unGZip(arr));
                    } else {
                        mTransferData.setData(arr);
                    }
                    mTransferData.init();
                    int leftLength = data.length - length - HEADRE_LENGTH;
                    if (leftLength > 0) {
                        byte[] leftBytes = new byte[leftLength];
                        System.arraycopy(data, total, leftBytes, 0, leftLength);
                        handleData(leftBytes);
                    }
                }
            } else {
                mTransferData.setData(data);
            }
        }
    }
```    
#### 连接保持（心跳机制）
一般APP实现连接保持的方式无非是采用应用层的心跳，通过心跳包的超时和其他条件(网络切换)来执行重连操作。那么问题来了:为什么要使用应用层心跳和如何设计应用层心跳。众所周知TCP协议是有KEEPALIVE这个设置选项，设置为KEEPALIVE后，客户端每隔N秒(默认是7200s)会向服务器发送一个发送心跳包。<br>
###### 但实际操作中我们更多的是使用应用层心跳。原因如下:
- KEEPALIVE对服务器负载压力比较大；
- 部分复杂情况下KEEPALIVE会失效，如路由器挂掉

##### 移动端在实际操作时为了节约流量和电量一般会在心跳包上做一些小优化：
- 精简心跳包，保证一个心跳包大小在10字节之内；
- 心跳包只在空闲时发送；
- 根据APP前后台状态调整心跳包间隔。

同时，也可以根据上一次或者上几次的网络情况，动态调整keep alive的时间。比如：如果前十次请求都没问题，则将间隔时间调长，反之，则将间隔时间调短。<br>
客户端的心跳机制在IM系统中扮演一个很重要的角色，为了兼顾消息的及时性和减少对服务器的压力，有很多可以优化的方向，我在这就不一一列举了，推荐大家阅读[Android微信智能心跳方案](https://mp.weixin.qq.com/s?__biz=MzAwNDY1ODY2OQ==&mid=207243549&idx=1&sn=4ebe4beb8123f1b5ab58810ac8bc5994&scene=1&key=dffc561732c22651a7a551503a3e34117fa2cfa4d0eabcfb2a8974dbc8bb9e038324ec7286885b7e855b9272585eee44&ascene=0&uin=MjMyNzA5NjUwMA%3D%3D&devicetype=iMac+MacBookPro11%2C1+OSX+OSX+10.10.5+build(14F27))。
#### 消息可达
在移动网络下，丢包，网络重连等情况非常之多，为了保证消息的可达，一般需要做消息回执和重发机制。参考易信，每条消息会最多会有3次重发，超时时间为15秒，同时在发送之前会检测当前连接状态，如果当前连接并没有正确建立，缓存消息且定时检查(每隔2秒检查一次，检查15次)。所以一条消息在最差的情况下会有2分钟左右的重试时间，以保证消息的可达。

因为重发的存在，接受端偶尔会收到重复消息，这种情况下就需要接收端进行去重。通用的做法是每条消息都戴上自己唯一的message id。

#### 文件上传优化
IM消息内包含大量的文件上传的需求，如何优化文件的上传就成了一个比较大的主题。
##### 常见有下面这些优化思路:
- 将上传流程提前:音频提供边录边传。朋友圈的图片进行预上传，选择图片后用户一般会进行文本输入，在这段时间内后台就可以默默将选好的图片进行上传；
- 提供闪电上传的方式:服务器根据MD5进行文件去重；
- 文件分块上传:因为移动网络丢包严重，将文件分块上传可以使得一个分组包含合理数量的TCP包，使得重试概率下降，重试代价变小，更容易上传到服务器；
- 支持断点续传。

### 客户端逻辑
以上所述基本上是要实现一个IM APP
所用到的一些技术方案，那么当服务器将用户的消息发送到客户端后，客户端具体是怎么处理的呢？

![](/image/im/flow.png)
###### 这里有几个点需要注意一下：

1.`离线消息`：因为我们的App允许多端登录，所以在处理离线消息的方案是用户进入会话列表页时会主动去拉取离线消息（很多APP都是这样处理的）。<br>
2.`历史消息`：历史消息及漫游记录，这个因为数据量太大，我们采用Http的形式去拉取。<br>
3.为了提高会话的显示速度，一些及时性不高的信息，会采用延后加载的方式，比如用户头像等信息。

#### 数据库设计
![](/image/im/fb.png)
一个完整的IM APP 必定会采用数据库缓存数据，你肯定不希望用户每次进入会话都需要从服务器去拉取消息，以及断网如果无法查看聊天记录。最简单的IM数据库也应该包括	`User`，`Conversation`，`Message`三个表，至于数据库如何设计，如何优化增删改查的速度，我就不多谈了。


以上，是我在开发IM SDK的过程中遇到的一些坑和总结的一些经验，浅薄之见，望大家别见笑，欢迎给出批评意见!
